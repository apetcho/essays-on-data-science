# Hiring and Interviewing Data Scientists

Last year in 2021, I hired two new teammates for my home team.
In 2023, I will be _potentially_ hiring for four more positions
(as long as the business situation doesn't change).
While I had helped hire data scientists in the past,
2021 was the first time I was primarily responsible for the role.
So when I first joined,
I thought I would have time to ease into the team lead part of the role.
But by day 2, July 21,
my manager Andrew sent me an MS Teams message saying,
"Hey, two positions for you are approved. Merry Christmas!"
I was both surprised and excited at the time.

## Criteria for Hiring

During this time, I had to develop the criteria for hiring.
Thankfully, our Talent Acquisition (TA) team had a process in place,
so I didn't have to design that half.
Our department head, Dave Johnson,
emphasized keeping the technical bar high
because we want to hire people who can hit the ground running straight away.
I also knew from previous experience
that I would enjoy working with bright individuals who were:

1. quick to learn,
2. thorough in thinking through problems,
3. capable of going deep into technical details,
4. had sufficient humility to accept feedback, and
5. were courageous enough to challenge my views.

The two we hired on the team fit these criteria perfectly.

At the same time, our Talent Acquisition team suggested that
we shouldn't necessarily hire someone who hits
all of the required technical skills for that level
but rather someone who has a majority of the skills necessary
and has shown the potential to learn the rest.

Why?
It's related to our psychology.
Most of us are motivated when we see a positive delta in our abilities,
receive affirmation that we are improving,
and can see the fruits of our labor in a finished product.
Hence, this advice ensures that our new hires
can hit the ground running while staying motivated longer.

Hiring takes time and is expensive;
the longer we can retain high-quality individuals within the team,
the better it is for continuity, morale, productivity, and the bottom line.
This news should encourage those
who would otherwise be discouraged from applying
for a role for which they are _not_ a perfect fit!

The broad categories of traits we were looking for include:

1. People skills
2. Communication skills
3. Scientific knowledge
4. Coding skills
5. Modelling skills

(Later on in this essay, we will go through those in more detail.)

## Team Interviews

We interview as a team, just as I experienced interviewing at Verily and NIBR
when I interviewed for my first data science role.
The team also serves as the hiring committee,
making a go/no-go recommendation to the hiring manager,
who makes the final call on whether to hire a candidate or to continue searching.
(Other companies may have a different practice.)

Interviewing as a team helps ensure that we cover a wide range of perspectives
and can cover a rich set of questions
covering both the technical skill and people dimensions.
As the hiring manager, I assemble the committee
and decide what aspect each hiring committee member will be interviewing.
Usually, I would pick potential close collaborators as interviewers,
which makes sense given the nature of our team's work being collaborative.

I was most comfortable interviewing for technical skills,
so I assigned the people dimension to my colleagues.
Because we are a team that works with scientists,
I also asked my colleagues to assess
the candidates' scientific knowledge within the interviewers' domain.
Let's explore these dimensions in a bit more detail below.

## People Skills

??? summary

    The people skills dimension in an assessment process involves
    evaluating a candidate's ability to handle difficult situations.
    This can be done by asking the candidate to describe
    a difficult situation they faced and how they handled it.
    The interviewer should look for several things in the candidate's story,
    including whether the situation was non-trivial and required handling,
    whether the candidate provided verifiable details about the situation,
    and whether they demonstrated desirable qualities such as
    humility, courage, resilience, and empathy.
    It is also important to consider any negative qualities
    that the candidate may have revealed, as they may be red or orange flags.

In assessing the people dimension,
we were looking for stories highlighting
a candidate's ability to handle testing situations.
So, why not ask for stories of their strengths?
Because adversity is one of the best ways of revealing character.
In adverse conditions, one's best (or worst) version of oneself gets unveiled.

When doing the interview, I would ask the candidate to tell me about a time
when they were in a difficult situation and how they handled it.
Here, then, is a set of questions that I would use to evaluate the story.

### People Skills Rubric

??? question "Was the situation sufficiently non-trivial to handle?"

    Trivial situations are neither interesting nor revealing.
    If the candidate can offer up a non-trivial situation they handled
    and explain how they handled it,
    that is a positive sign.
    On the other hand, if they can only offer up trivial situations,
    then there is a chance they may not have sufficient battle scars
    to act maturely when faced with a non-trivial situation.

??? question "How much verifiable detail did the candidate provide in the story?"

    Details indicate ownership over the situation,
    and verifiability means we can check the details for truthfulness.
    Therefore, it's a positive sign if
    the candidate can offer up a story with sufficient details,
    such as the nature of the situation and
    how the people involved would be affected by their actions and decisions.
    Additionally, if the details are self-consistent,
    that's a plus sign too!

??? question "Did the candidate reveal generally desirable qualities, such as humility, courage, resilience, and empathy? Which specific details revealed these qualities?"

    We can gain an accurate picture of the candidate's character
    as long as the candidate provides details.
    Hence, details matter!
    Additionally, the qualities mentioned above
    are what we want to see in our teammates.

??? question "Apart from these qualities, what other positive qualities did the candidate reveal? Or did they reveal negative qualities instead that may turn out to be orange or red flags? What specific details revealed these qualities?"

    These are the qualities that we want to avoid in our teammates.
    If the candidate reveals negative attributes,
    we need to assess whether they are red or orange flags
    or whether cultural differences led to the negative perception.

## Communication Skills

??? summary

    Communication skills are important in our roles
    because effective communication is necessary
    for collaborating with others
    and ensuring that one's work is understood.
    To assess communication skills,
    candidates may be asked to present
    a data science project they have worked on,
    with the presentation followed by a question and answer session.
    During the presentation,
    we consider

    1. whether we can summarize the candidate's problem statement, methodology, and key findings in a few bullet points,
    2. how engaged the audience was during the presentation, and
    3. any aesthetically pleasing aspects of the presentation.

Communication skills matter.
Our work is irrelevant if our collaborators do not understand it.
Hence, we need to ensure that our candidates can communicate effectively
to a wide range of audiences - from the technical to the non-technical,
from junior members to senior leadership,
and from the scientific to the business.

To assess communication skills, we often use a seminar format.
Here, we ask the candidate to present a data science project they have worked on.
Sometimes it is their thesis work;
at other times, it is a project they have worked on in their current role.
Usually, there is a 30-40 minute presentation
followed by additional time, interleaved or at the end, for Q&A.
When assessing a candidate's communication skills,
this is the rubric that I usually go for.

### Communication Skills Rubric

??? question "Based on the presented material, am I able to summarize the candidate's problem statement, methodology, and key findings in 3 bullet points?"

    This question places the onus on me to put in
    a best-faith effort at understanding the candidate's presentation.
    If I can summarize the candidate's presentation in 3 bullet points,
    that's an immediate positive sign.
    That said, even if I can't summarize the presentation in 3 bullet points,
    I can use follow-up questions to attempt to understand the presentation better.
    If, after a good-faith follow-up,
    the candidate still cannot explain their work in a way that I can understand,
    only then do we have a red flag.

??? question "How engaged were the audience during the candidate's presentation?"

    High engagement is a positive aggregate sign;
    while it doesn't necessarily reveal specific traits,
    it usually means the candidate is able to hold the audience's attention
    through a combination of:

    1. A clear and compelling problem statement that resonates,
    2. A methodlogy that may be non-trivial but still logical,
    3. A conclusion that is _interesting_ and potentially _relevant_ to the audience, and
    4. Effective visual storytelling by the candidate themselves.

??? question "What were aesthetically pleasing aspects of the presentation, if any?"

    This question is subjective, but is of at least second order importance to me.
    I would like to work with individuals who have a good sense of design
    and, more importantly, have the capability to execute on that design.
    Having a pleasant presentation aesthetic,
    such as having pixel-perfectly aligned shapes,
    is a second order indicator of attention to detail.

## Scientific Knowledge

??? summary

    Scientific knowledge is important in our roles
    because it lubricates communication with scientists
    and enables us to follow along in scientific discussions.
    For junior candidates,
    this may mean having a solid understanding of at least one domain of science,
    while senior candidates should have knowledge of multiple domains.
    Familiarity with the experiment design process is also important,
    as data scientists in some fields may need to work with experimental scientists.
    To assess scientific knowledge,
    an interviewer might ask a candidate to describe
    a scientific experiment they helped to design
    and evaluate their response using a rubric that considers
    the candidate's understanding of the backing theory for the experiment,
    the level of detail provided about the experiment,
    and the limitations of the experiment design.

Because we are a team that works with scientists,
we also need to ensure that our candidates can communicate with scientists,
which heavily implies that they need a solid scientific knowledge base.

For junior candidates, such as those fresh from a Master's or Bachelor's program,
this means they need to have sufficient knowledge over at least one domain of science,
such as biochemistry, analytical chemistry, or immunology,
depending on the expectations of the role.

Senior candidates, such as those who have finished a Ph.D. or post-doc training,
should know more than one domain of science.
Both levels should also be able to follow along in scientific discussions
and be courageous enough to raise questions when they don't understand something.

Because our work involves working with experimental scientists,
familiarity with the experiment design process is also essential.
Hence, we place a premium on
knowing how to design an experiment and institute appropriate controls,
all while articulating the limitations of that experiment.
A data scientist in our domain who has those qualities
will be able to win the trust of our wet lab scientist colleagues.

To assess scientific knowledge, particularly experimental design knowledge,
I would ask the candidate to describe a scientific experiment they helped to design
and see how well they could articulate the details of the experiment.
Then, I would use the following questions to evaluate their response.

### Scientific Knowledge Rubric

??? question "How much backing theory for the experiment did the candidate reveal?"

    The candidate should be able to connect the experiment to the theory.
    As a trivial example, if the experiment contains a polymerase chain reaction (PCR) step,
    they should be ready to answer questions about how PCR works at some level,
    with a greater expectation of detail for candidates for higher seniority levels.
    An alternative example would be a synthesis of knowledge,
    where a candidate describes a follow-up experiment to one that someone else designed.

??? question "How much detail did the candidate provide about the experiment?"

    This question checks a candidate's proficiency in experiment design.
    The presence of detail helps us build confidence that the candidate actually participated in the experiment design.
    They should be able to describe the experimental controls and what they control for.
    They should be familiar with the
    various axes of variation in the experiment (such as time or temperature)
    and how we expect these axes to affect the outcome of the experiment.

??? question "Did the candidate highlight particular limitations of the experiment design?"

    No experiment will give us the answers we need for scientific inquiry.
    As such, the candidate should offer up the limitations of the experiment.
    For example, did they highlight limitations of the controls,
    where physical limitations may occur in the experiment,
    and how they would address those limitations in a future experiment?

## Coding Skills

??? summary

    Coding skills are important in our work
    because we expect code that is both readable and maintainable.
    To assess coding skills,
    an interviewer might ask the candidate to bring
    a piece of code they are proud of to the interview
    and review it code-review style.
    During the code review,
    the interviewer may ask the candidate
    questions about the purpose of the code,
    how it is organized,
    and any tradeoffs made in the code organization.
    They may also ask the candidate about any dissatisfaction with the code
    and how they would improve it in the future.
    The interviewer should consider
    whether the candidate offered details about the code without prompting,
    how well-organized and well-documented the code is,
    and whether the code reflects idiomatic domain knowledge.

Coding skill is another axis along which we assess candidates.
Because ["data science can't be done in a GUI"][hadley],
we need to ensure that our candidates can write
both readable and maintainable code.
After all, even though our primary value add is modeling skill,
we still need to write code that surrounds the model
so that we can make the model useful.
Hence, it is crucial to effectively asses a candidate's coding skills.

How do we assess coding skills?
If a candidate interests me,
I will proactively look out for their profile on GitHub
to see what kind of code they've put out there.
One well-maintained codebase is a positive;
not having one is **not** a negative,
as I understand not everybody has the privilege of time to maintain a codebase.

I have found one way to deeply evaluate a candidate's coding skills,
which I would like to share here.
In this interview question,
I would ask the candidate to bring a piece of code
they are particularly proud of to the interview.
We would then go through the code together, code-review style.

We intentionally ask candidates to pick the code they are proud of,
which gives them a home-court advantage.
They should be able to explain their code better than anyone else.
They also won't need to live up to an imagined standard of excellence.
Additionally, because this is their best work,
we gain a glimpse into what standards of excellence they hold themselves to.

During the code review,
I would ask the candidate questions about the code.
These are some of the questions that I would cover:

1. What's the purpose of the code?
2. How is it organized? Are there other plausible ways of organizing the code?
3. What tradeoffs did the candidate make in choosing that particular code organization?
4. Are there parts of the code that remain dissatisfactory, and if so, how would you improve it for the future?

That 4th question is particularly revealing.
No code is going to be perfect, even my own.
As such, if a candidate answers "no" to that question,
then I would be wary of their coding skills.
A "no" answer usually betrays a Dunning-Kruger effect,
where the candidate thinks they are better than they actually are.

That said, even a "yes" answer with superficial or scant details
betrays a lack of thought into the code.
Even if the code is, in my own eyes, very well-written,
I would still expect the candidate to have some ideas
on **where** the code could be extended for a logical expansion of use cases,
refactored, or better tested.
If the candidate cannot provide details on these ideas,
it would betray their shallow thinking
about the problem for which they wrote the code.

In the next section, I'll describe my rubric for assessing coding skills.

### Coding Skills Rubric

??? question "Did the candidate offer aforementioned details without prompting?"

    This is a sign of experience;
    they know how to handle a code review, which we often do,
    and are usually confident in their knowledge of
    their code's strengths and weaknesses.

??? question "How well-organized was the code? Does it reflect idiomatic domain knowledge, and if so, how?"

    Organizing code logically is a sign of
    thoroughly thinking through the problem domain.
    Conversely, messy code usually suggests that
    the candidate is not well-versed in the problem domain
    and hence does not have a well-formed opinion on
    how they can organize code to match their domain knowledge.
    Additionally, because code is read more than written,
    a well-organized library of code will be easily reusable by others,
    thereby giving our team a leverage multiplier in the long run.

??? question "How well-documented was the code? In what ways does the documentation enable reusability of the code?"

    Documentation is a sign of thoughtfulness.
    In executing our projects,
    we consider the problem at hand
    _and_ the reusability of the code in adjacent problem spaces.
    Documentation is crucial here.
    Without good documentation,
    future colleagues would have difficulty understanding the code
    and how to use it.

??? question "Did the candidate exhibit defensive behaviors during the code review?"

    A positive answer to this question is a red flag for us.
    We want to hire people who are confident in their skills
    but humble enough to accept feedback.
    Defensive behaviors shut down feedback,
    leading to a poor environment for learning and improvement.

??? question "How strong were reasons for certain design choices over others?"

    This question gives us a sense of the candidate's thought process.
    Are they thorough in their thinking, or do they rush to a solution?
    Because our team is building out machine learning systems,
    we must be careful about our design choices at each step.
    Therefore, if a candidate does not demonstrate
    thinking thoroughly through their design choices,
    then it means we will need to spend time and effort coaching this habit.

## Modeling Skills

??? summary

    Modeling skills are an important part of our roles.
    Modeling involves the ability to pick and build
    computational or mathematical models to solve problems.
    This can involve using explicit mathematical models,
    such as differential equations or state space models,
    which I call "physics-style" mechanistic modeling.
    To assess a candidate's modeling skills,
    we can use presentations or seminars,
    during which the candidate presents a problem they are working on
    and explains their modeling approach.
    Questions can be asked about the modeling process,
    such as the candidate's ability to consider alternative models
    and mapping the parameters of the model onto explainable aspects of the problem.
    When evaluating a candidate who presents a neural network model,
    the ability to explain the model architecture in detail
    and understand the inductive biases of the model
    and their suitability for the problem at hand is important.

The final aspect of our roles as data scientists
is picking and building computational or mathematical models
that help us solve the problems we are working on.
By their very nature, models are an abstract simplification of the real world.
Then, modeling skill is the artful ability to figure out
what needs to be abstracted while translating that into code.

??? note "What do I mean by modeling skills?"

    I'd like to disambiguate what I don't mean by modeling skills.
    If you're used to building _predictive_ models of the world,
    you might say something along the lines of,
    "XGBoost is all you need for tabular data,
    variants of UNet for image machine learning,
    and a Transformer for sequence data."
    Or you might be used to trying several `scikit-learn` models
    and deciding on one based on cross-validation.
    In my mind, this is not skillful modeling;
    it's just following a recipe.
    By modeling skill, I am referring to the ability to do
    physics-style **mechanistic modeling**.

    As a team embedded in a larger _scientific research_ organization,
    there is something deeply dissatisfying with only being able to make predictive models.
    Being able to build explicit mathematical models is what I mean by modeling skill,
    whether that involves the use of differential equations,
    state space models,
    or other forms of mechanistic modeling.

Now, how do we assess modeling skills?
Usually, we do this through a seminar or presentation.
First, the candidate would present a problem they are working on
and then explain how they are modeling the problem.
Then, during the seminar,
we would ask questions about the modeling process
and use the candidates' answers to assess their modeling skills.
While handy, I think presentations may be insufficient alone;
often, I have found that we need to dig deeper during the 1:1 sessions
in order to tease out the necessary details to assess modeling skills.

### Modeling Skills Rubric

??? question "What alternative models or modeling approaches did the candidate consider, and why were they not chosen?"

    If the candidate can provide a solid and logical reason for their final model choice
    while contrasting it against other choices,
    then it means they are familiar with a broad range of approaches,
    which is a sign of a well-rounded modeling skillset.

    I would expect this trait in someone who is a Data Scientist-level hire.
    For a Research Associate-level hire, this would be less of an expectation,
    but if the candidate possesses this trait, I would be impressed.

    Someone who can only come up with one model option to solve a problem
    needs more training in modeling.

    Finally, if the candidate can't explain why they chose a particular model,
    that is a red flag, and I would be wary of their modeling skills.

??? question "If applicable, how clear was the candidate in mapping the parameters of the model onto explainable aspects of the problem?"

    If the model involved was an explicit model of a system,
    such as a hidden Markov model or a linear model,
    I would expect the candidate to explain how the parameters of the model
    map onto the key questions being answered.
    For example, what does the slope parameter in a linear model mean for this problem?
    How do we interpret the transition matrix in an HMM?

??? question "If the candidate presented a neural network model, can the candidate explain the model architecture in detail without jargon? Can they articulate the inductive biases of the model and contrast those biases' suitability for the problem at hand?"

    In particular, I would expect the candidate to explain
    how they arrived at their preferred model architecture
    and how they _ruled out_ other architectures.
    Too often, I have seen candidates pick an architecture they are familiar with
    or feel is popular or hot
    without thinking through their choice's pros and cons.
    (The worst I saw was using
    a convolutional neural network on tabular data for no good reason --
    in a journal article that I reviewed.)
    A strong candidate would compare and contrast
    the inductive biases of various architectures.

## Final Evaluation

As hiring managers,
we have tradeoffs to consider
when evaluating the candidate along the four dimensions above.
It is exceedingly rare to find a candidate who is strong in all four dimensions.
Beyond raw execution ability,
complementarity with the rest of the team also matters.
Therefore, we must decide which dimensions we are most willing to coach on
and in which dimensions we expect a candidate to be well-equipped.

Here is an example of a set of questions I would ask myself about a candidate.

??? question "Am I capable of coaching them something new?"

    I'm more inclined to coach on coding skills than modeling skills,
    so I would expect a candidate to come in with a robust modeling skillset;
    my role would be to refine their ability to write software.
    Hence, I would prioritize candidates according to that criteria.
    (Other hiring managers would prioritize differently.)
    Additionally, I am willing to coach on communication skills
    but less on domain knowledge,
    so I expect a candidate to come in knowing their science fundamentals well.

??? question "Does the candidate come equipped with special modeling skills that are complementary to the team's current skill sets?"

    Beyond the role's requirements,
    we would also like to bring additional skill sets to the team.
    Doing so lets us tackle a broader range of problems
    (and therefore deliver more value),
    foster a learning environment with a diverse breadth of skills,
    and crucially, build a more resilient team,
    especially if one of our team members leaves.

??? question "Will the candidate work well with other teammates and collaborators?"

    For example, are they able to communicate well with others?
    Do they leave others feeling more confused than clear?
    If they display a sense of humor, does it mesh well with the team,
    or does it come off as abrasive?

    Do they have the confidence to execute their ideas in a group setting
    with a track record of successful execution to back it up?

## General Lessons Learned

Here are some general lessons that I learned from the hiring process.
These should be beneficial to both interviewers and interviewees.

### Details matter

Firstly, details matter!
A pattern you may see above is that
I expect to see many details relevant to the skill under assessment.
An apocryphal story I read before stated that
details are how Elon Musk tells if a candidate is faking it.
While I disagree with many of Elon Musk's ways,
this is a handy lesson for interviewers and interviewees.

**For interviewers**, it is vital to dig into details.
The more information you can fish out,
the better handle you'll have on your candidate's capabilities,
and you'll be less likely to make a poor hire.
**For interviewees**, conversely, it is important to be forthcoming with details.
The more you're able to provide details to your interviewer,
especially the ones critical to the job,
the more accurately your interviewer will be able to evaluate you.
Your challenge, as a candidate, is to be concise _and_ detailed!
You _don't_ want to overwhelm your interviewer with irrelevant details.
**For both parties,** the best way to avoid doing so is to treat the interview as
a conversation rather than a presentation,
where you share information in a back-and-forth manner.

### Local context matters

Secondly, the team's local context matters!
I see teams as continual works-in-progress, puzzles with missing pieces.
The missing piece has to _fit_ and _enhance_ a team.
And by that _fit_, I don't just mean the fuzzy cultural fit
that can be used as a smokescreen for rejecting a validly great candidate.

We need to consider technical skills and
what the hiring manager and the rest of the team are willing to coach.
Ideally, these should be complementary for the benefit of the team _and_ new hire.

We also need to consider whether the candidate's mix of skills
can bring a new dimension to the team's capabilities.
For example, if the team needs someone with deep learning experience
and a candidate also brings probabilistic modeling skills
that the team currently lacks,
that candidate would bring a unique new capability to the team.
Therefore, we would favor that candidate
over another who _only_ brings deep learning experience.

What does this mean for candidates?
First, it can mean that even though you're thoroughly qualified for a role,
you might not be the final choice because of the local context of a team.

For example, a hiring manager may be willing to coach you on skills you're already strong in,
which means they would not be as great for you as a professional development coach.
(This is an essential expectation for you to have for your manager!)
Therefore, from this perspective only,
it would be to your benefit to _not_ be accepted for the role
and to continue searching elsewhere.

Or it could mean that your skillsets were an excellent match,
but another candidate came in with an enhancer skillset
that complemented what the team already had.
Of course, from this perspective,
that other candidate only wowed the hiring manager and the team
with their extra skillsets that you didn't have,
and they will be valued accordingly.
Nonetheless, you should still be confident in your capabilities
and continue to search for a team to which you can be a special sauce contributor.
That will give you the confidence to know that you possess something special for the team
and will be valued accordingly.

### Expectations mapped to level

Where I work, we have three broad levels of technical folks, in increasing order:

1. Research Associates (3 levels, including Senior and Principal)
2. Data scientists (3 levels, including Senior and Principal)
3. Fellows (3 levels, including Senior and Distinguished)

Research Associate ranks are usually for fresh graduates out of Bachelors' or Masters' programs,
with the Senior and Principal ranks for longer industry experiences.
Data scientists are for those newly graduated from Ph.D. programs
or just finishing their post-doc training without industry experience
or Master's graduates with prior industry experience in a related role.
Here, the Senior and Principal ranks are generally for
existing data scientists with a track record of accomplishment in a prior industry role,
regardless of prior educational training.
"Fellow" ranks are for highly skilled and experienced individuals
who bring technical, domain, and people leadership skills to the table.
(Thought leadership is the summary term for all three.)

We calibrate our expectations for each level.
For example, our expectations of Data Scientists,
who either have extensive industry experience or have completed doctoral training,
are much higher than that of a Research Associate.
We expect them to be much more mature in their people skills,
more polished in their communication skills,
and possess greater depth and breadth of domain knowledge.
On the other hand,
Fellows need to be concerned with technical leadership,
have a firm grasp of our work's impact to research productivity,
and be able to mentor and coach others on effective work practices.

When it comes to coding skills,
I think we recognize that most of our candidates fresh from academia
would not be as polished as those with industry experience.
My experience played out uniquely;
I picked up software development skills during my graduate school days,
thanks to my involvement in the open-source community.
However, most of my peers did not have the same experience,
and I am seeing the same situation six years after graduation.
Therefore, at the time of writing (December 2022),
I consider any demonstrated software development skill
to be a differentiating attribute for a candidate.
Finally, with increasing seniority,
we expect stronger opinions on structuring and organizing code,
more extensive experience with code review
and a more nuanced understanding of software development best practices.

When it comes to modeling skills,
I would expect a Research Associate-level candidate
to be well-trained in a few modeling techniques
(especially those taught in regular university curricula),
know where to apply those models, and compare and contrast their use cases.
On the other hand,
data scientists should be able to apply a broader range of modeling techniques
(covering both mechanistic and data-driven models),
and they should be able to dive into the math behind them.
With increasing seniority,
candidates should also have a more opinionated philosophy on modeling strategy.
For example, where to use a mechanistic model vs. a data-driven model,
or, my favorite,
being able to cast any modeling problem through the lens of probabilistic modeling.

## Continual Challenges when Interviewing

There is still one area that I feel is challenging to assess:
how fast a candidate can learn a new thing.
I have tried asking questions, such as:

- How do you learn a new technical skill?
- What is your process for learning a new topic?

However, I tend to get generic answers, such as:-

- Asking other people.
- Reading papers.
- Trying out an example with code.

None of those answers really get to the heart of what I'm trying to ask:
how does the candidate master a new skill _quickly_?
I probably need a better way to ask the question.

## Summary

This essay is the culmination of reflecting on my hiring experiences
since I joined the industry in 2017.
I wrote it down with three goals in mind:

1. To record what I've learned from my hiring experiences for future reference,
2. To help others who may find the hiring process to be opaque or mystical, and
3. To serve as a conveniently sharable record of my hiring thought process for my colleagues.

If you've found this essay useful as a hiring manager,
please consider doing two things:

1. Sharing it with your peers and colleagues so that they may benefit, and
2. Leave me a note on the repository's [discussion forum][discussion] to let me know what you think.

If, as an interviewee, you are starting to feel intimidated at the process,
please reconsider your feelings!
I've peeled back the curtain on what I look for in a candidate
precisely to help you, the interviewee, understand better what we're looking for.
It should help you better prepare for an interview,
at least, for an interview with me and my team.
Likewise, if you've found the essay useful,
please consider sharing it with your peers,
and let me know your thoughts and questions on the [discussion forum][discussion].

[hadley]: https://www.youtube.com/watch?v=cpbtcsGE0OA

[discussion]: https://github.com/ericmjl/essays-on-data-science/discussions
